---
title: "How Will SLR Increase Risk of Tidal Flooding?"
author: "Curtis C. Bohlen"
date: "December 16, 2020"
output:
  github_document:
    toc: true
---

<img
    src="https://www.cascobayestuary.org/wp-content/uploads/2014/04/logo_sm.jpg"
    style="position:absolute;top:10px;right:50px;" />

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center',
                      fig.width = 5, fig.height = 3,
                      collapse = TRUE, comment = "#>")
```
 
# Introduction
One reviewer of our draft Climate Change chapter pointed out that for all
weather-related events, we presented data on the changing frequency of "events" 
over time, including hot days, cold days, large storms, etc.  They suggested we 
show similar graphics showing changes in frequency of tidal flooding events.
We prepared that graphic in
[Tidal_Flooding_Events.Rmd](Graphics/Tidal_Flooding_Events.Rmd]).

In this notebook, we to put that graphic in context by looking at estimates of
future tidal flooding risk under a few SLR scenarios.  Our goal is to be able to
say that analysis or simulation suggests an x% increase in frequency of flooding
with a Y foot increase in SLR.

In our analysis, we follow Maine Geological Survey's practice of declaring a
tidal flooding event whenever tidal observations exceed the current "Highest
Astronomical Tide" or HAT level, which is 11.95 feet, or 3.640 meters, above
mean lower low water (MLLW) at Providence.

Maine Geological Survey has estimated future flooding risk by adding
a SLR value (0.8 foot and 1.5 feet) to the historical record, and showing how 
frequent flooding would have been under SLR conditions. This provides a ready
estimate of impact of SLR on frequency of flooding.  They do not appear to
provide quantitative estimates of change of frequency of flooding.

Details of their method and a data viewer for different locations in Maine are
available at the 
[Maine Geological Survey sea level rise data viewer](https://mgs-collect.site/slr_ticker/slr_dashboard.html).

We repeat their analysis for selected SLR scenarios ourselves, over a fixed
period of time (the past 20 years) and estimate percentage change in flooding
under SLR.  Our results will differ from theirs, as we count days per year with
flooding, while they count hours.

We then go on to simulate flooding histories based on the 19 year tidal epoch,
and examining predicted flood frequencies using three different models under
one foot, two foot, and three foot SLR scenarios.

# Import Libraries
```{r libraries}
library(tidyverse)
library(readr)

library(data.table)  # for fread to read large files

library(moments)  # for skewness and kurtosis; we could calculate, but why?
library(forecast) # for auto.arima()
library(lubridate)

library(CBEPgraphics)
load_cbep_fonts()
theme_set(theme_cbep())
```

# Import Data
Our primary source data is hourly data on observed and predicted water levels at
the Providence tide station (Station 8418150).  We accessed these data using small
python scripts to download and assemble consistent data from the NOAA Tides and
Currents API.  Details are provided in the "Original_Data" folder.

```{r folders}
sibfldnm <- 'Original_Data'
parent <- dirname(getwd())
sibling <- file.path(parent,sibfldnm)

#dir.create(file.path(getwd(), 'figures'), showWarnings = FALSE)
```

Loading this file with read_csv was crashing....

```{r load_observed_data}
fn <- 'providence_tides_hourly.csv'
fpath <- file.path(sibling, fn)

# observed_data  <- read_csv(fpath)
observed_data  <- fread(fpath)

observed_data <-  as_tibble(observed_data) %>%
  rename(MLLW = `Water Level`,
         theDate =`Date`) %>%
  filter(! is.na(MLLW)) %>%
  mutate(DateTime = ymd_hm(DateTime),
         theDate = as.Date(theDate),
         Year = as.numeric(format(theDate, '%Y')),
         MLLW_ft = MLLW * 3.28084,
         Exceeds = MLLW > 1.987)
```

```{r}
observed_data <- observed_data %>%
  mutate(Hour  = as.numeric(format(DateTime, '%H')),
         Month = as.numeric(format(theDate, '%m')),
         Day   = as.numeric(format(theDate, '%d')),
         Year  = as.numeric(format(theDate, '%Y'))) %>%
  select(-Sigma)
```

For some reason, loading this file did not crash R.
```{r load_predicted_data}
fn <- 'providence_tides_hourly_predicts.csv'
fpath <- file.path(sibling, fn)

predict_data  <- read_csv(fpath, col_types = cols(Time = col_time('%H:%M'))) %>%
  rename(theDate =`Date`) %>%
  mutate(Hour  = as.numeric(format(DateTime, '%H')),
         Month = as.numeric(format(theDate, '%m')),
         Day   = as.numeric(format(theDate, '%d')),
         Year  = as.numeric(format(theDate, '%Y'))) %>%
  select(-DateTime, -theDate, -Time)
```

## Combine Data
The number of predicted and observed values are not **quite** the same, so we
need to make sure we merge data appropriately. We merge by Year, Month, Day,
and Hour.  We could use DateTime, but we have not checked if both
files use the same conventions throughout.  This is likely more robust.
```{r combine_data}
combined <- observed_data %>%
  inner_join(predict_data,by = c("Year",  "Month", "Day", "Hour"))

## Calculate Deviations Between Predicted and Observed
combined <- combined %>%
  mutate(deviation = MLLW - Prediction)
```

## Cleanup
```{r cleanup-1}
rm(summ, observed_data)
```

## Long Term Trend
```{r trend_in_deviations_plot}
ggplot(combined, aes(DateTime, deviation)) +
  geom_point(size = 0.01, alpha = 0.05, color = 'yellow') +
  geom_smooth(method = 'lm')
```

```{r initial_lm}
test <- lm(deviation ~ theDate, data = combined)
coef(test)
```
We do not show error estimates of this regression, as observations are
highly autocorrelated, and estimated error would be misleading, but the slope
is meaningful. The implied rate of annual increase in deviations is 

```{r est_annual_increase}
unname(round(coef(test)[2]*1000*365.25,2))
```
Here, the regression slope suggests a 2.39 mm annual rate of increase in
the deviations between predicted and observed water levels.  Not coincidentally,
that value is remarkably close to the long-term estimate of SLR at the Providence
station.

# The Tidal Epoch
Tidal predictions are defined in terms of a specific 19 year long "Tidal Epoch."
The astronomical alignments of sun, moon, and earth repeat (at least closely
enough for tidal prediction) every nineteen years, and tides are predicted based
on astronomical processes. Tidal "Predictions" are based on  a complex periodic
function that is parameterized by "harmonic constituents", themselves calculated
based on observed tidal elevations over the 19 year Tidal Epoch.

The current tidal epoch for Providence is 1983-2001. (According to the NOAA web
page for the station.  The epoch is provided on the datums sub-page.)

For our purposes, the key insight is that tidal predictions are effectively a
nineteen year-long periodic function, and thus do not take into account changes
in sea level. Consequently, the deviations from tidal predictions we calculated
are not stationary.

However, during the Tidal Epoch, the average error of prediction should be zero
(or very close to zero).  Our best understanding of the distribution of
deviations from (astronomical) predicted tide elevations, therefore, would come
from looking at deviations during that tidal epoch, when deviations due to 
changing sea level are minimized.

```{r filter_to_epoch}
epoch <- combined %>%
  filter(Year> 1982 & Year < 2002)
```

# Evaluating Data for Simulations
We continue, working with data where tide levels are expressed in meters 
above MLW.  We focus on the Tidal Epoch. Here we evaluate the data in terms of 
how best to simulate future flooding events.

## Trend During the Tidal Epoch
```{r est_annual_increase_epoch}
test <- lm(deviation ~ theDate, data = epoch)
coef(test)
unname(round(coef(test)[2]*1000*365.25,2))
```
We do not show error estimates of this regression either, but it suggests a
change in deviations slightly higher than for the entire record at about 3 mm
a year.

## How Many Flood Events per Year?
As a check on our methods, it's nice to know how many flood events actually
occurred during the official tidal Epoch.
```{r}
flood = 1.987
epoch %>% 
  group_by(theDate)  %>%
  summarize(exceeded = any(MLLW> flood),
            .groups = 'drop') %>%
    summarize(days = sum(! is.na(exceeded)),
              floods = sum(exceeded, na.rm = TRUE),
              floods_p_yr = 365.25 * floods/days) %>%
    pull(floods_p_yr)
```
Using slightly different methods, we get a slightly different numbers. The prior method averages floods across the whole period and scales to the length of the average year.  Here we average the number of floods per year, either scaled or
or unscaled by the number of days with complete observations. 
```{r}
HAT <- 1.987
epoch %>%
  group_by(theDate) %>%
  summarize(Year = first(Year),
            exceeded = any(MLLW > HAT),
            n = sum(! is.na(exceeded)),    # Count days with ANY data...
            .groups = 'drop') %>%
  group_by(Year) %>%
  summarize(days = sum(n),
            floods = sum(exceeded),
            floods_p_d = floods / days,
            floods_p_yr = floods_p_d * 365.4,
            .groups = 'drop') %>%
  summarize(across(contains('floods'), mean, na.rm = TRUE))
```

So, over the official tidal epoch, we saw o the order of three and a third 
flood events a year.

### Distribution of Deviations
We need to figure out how best to model future deviations between observed and
predicted tide levels.  In particular, we need to identify strategies for 
creating a reasonable "random" version of deviations so that we can simulate
future conditions.

**Values of Tide Levels in the rest of the Notebook  are in meters.**
```{r hist_deviations}
ggplot() +
  geom_histogram(aes(x = deviation, y = ..density..),
                 bins = 100, data = epoch) +
  geom_density(aes(x = deviation), color = 'red', data = epoch) +
  stat_function(fun = dnorm,
                args = list(mean = mean(epoch$deviation, na.rm = TRUE),
                                   sd = sd(epoch$deviation, na.rm = TRUE)),
                color = 'blue') +
  geom_vline (xintercept = 0, color = 'red')
```
So, deviations are "close" to normally distributed, or at least bell shaped,
with mean close to zero.  A mean close to zero is not due to chance, but the
nearly normal distribution reflects something about the nature of atmospheric
and other non-astronomical effects on local sea level.

#### Calculate the Moments of the Distribution
```{r moments_deviations_epoch}
mean(epoch$deviation, na.rm = TRUE)
sd(epoch$deviation, na.rm = TRUE)
skewness(epoch$deviation, na.rm=TRUE)
kurtosis(epoch$deviation, na.rm = TRUE)
```

Formal statistical tests will do us little good here.  With this much data,
nothing ever matches theoretical distributions exactly.  This is a matter for
judgment, not testing.

##### Skewness
A non-zero skew means the distribution is not symmetrical about its mean.  A
positive skew suggests a few extra observations below the mean, and more
extreme observations above the mean.  The median is lower than the mean.  The
upper tail is longer than the lower tail.

We can confirm that somewhat more observations are below then mean than above.
```{r frac_below_mean_deviations_epoch}
below <- sum(epoch$deviation < mean(epoch$deviation, na.rm = TRUE),
             na.rm = TRUE)
above <- sum(epoch$deviation > mean(epoch$deviation, na.rm = TRUE),
             na.rm = TRUE)
below/(below+above)
```

So we have a couple of percent more points below the mean than a normal
distribution. We must have a few extreme values above the mean to balance
the distribution about the mean.

##### Kurtosis
Positive kurtosis suggests a "fat tailed" distribution.  That is, there are more
observations in the center and tails of the distribution than would be true of a
normal distribution with similar standard deviation. Again, we have a few more
extreme values.

#### Conclusion
We are interested in the extremes of this distribution -- or the frequency of
extremes.  The effects of skewness and  kurtosis on frequency of extreme values
reinforce each other. There will be more extreme values than under a normal 
distribution.

Simulation with a normal distribution would probably be a mistake. We need to
either use a distribution where we can tune the skewness and kurtosis, resample
from the observed distribution of deviations, or use more sophisticated tools.

### Temporal Autocorrelation
```{r acf_3}
acf(epoch$deviation, na.action = na.pass, lag.max = 24*5)
```

So, we see high autocorrelation, with a relatively weak (at least compared to Portland, Maine) roughly 12 hour (tidal) cycle imposed
over a gradual decay.  Autocorrelation remains present over a period of days. 
It's not obvious whether that 12 hour pattern is physical or a mathematical
artifact of how the predicted tides are calculated.

```{r pacf_3}
pacf(epoch$deviation, na.action = na.pass)
```
Partial autocorrelations at lag 1 are high, so the core of any ARIMA model
must be a lag 1 term, probably an AR(1) process. There is also some
quasi-periodicity, perhaps at a three hour periodicity, but the structure is not clear.

#### Conclusions
We can't assume a simulation that does not address autocorrelation will
be adequate.

### Correlations Between redictions and Deviations
```{r}
ggplot(epoch, aes(Prediction, deviation)) +
  geom_point() +
  geom_smooth(method = 'lm')
```
```{r}
cor.test(epoch$Prediction, epoch$deviation)
```

#### Conclusion
There is no clear evidence that there are correlations between deviations the
predicted tide and observed deviations in the historic record. (While this is
not unreasonable, since predictions are based on observed tidal elevations,
it is probably not a mathematical identity based on forecast methods....)

# Reprise the MGS Analysis
We reprise the analysis conducted by Maine Geological Survey.  MGS added fixed 
estimates of SLR to the historic record, and counted hours of time with coastal
flooding, defined as tidal elevation exceeding the HAT elevation.

We take the same approach to identify hours with coastal flooding under one
foot, two foot, and three foot SLR scenarios, but then count **days** in which flooding occurs. We focus on impact
of moderate SLR on coastal flooding compared to the frequency of flooding
observed over the past 20 years.

We use the Tidyverse for these relatively simple calculations, relying on
methods developed to produce graphics showing number of days with flooding
over the period of record. 

```{r mgs_analysis}
HAT <- 1.987
slr_annual <- epoch %>%
  mutate(slr_1 = MLLW + .3048,  # 1 foot
         slr_2 = MLLW + .6096,  # 2 foot
         slr_3 = MLLW + .9144,  # 3 foot
         exceeds_0 = MLLW    > HAT, 
         exceeds_1 = slr_1   > HAT,
         exceeds_2 = slr_2   > HAT,
         exceeds_3 = slr_3   > HAT) %>%
  group_by(theDate) %>%
  summarize(Year = first(Year),
            exceeded_0 = any(exceeds_0, na.rm = TRUE),
            exceeded_1 = any(exceeds_1, na.rm = TRUE),
            exceeded_2 = any(exceeds_2, na.rm = TRUE),
            exceeded_3 = any(exceeds_3, na.rm = TRUE),
            n = sum(! is.na(Exceeds)),
            .groups = 'drop') %>%
  group_by(Year) %>%
  summarize(days = n(),
            floods_0 = sum(exceeded_0),
            floods_1 = sum(exceeded_1),
            floods_2 = sum(exceeded_2),
            floods_3 = sum(exceeded_3),
            floods_0_py = 365.25 * floods_0 / days,
            floods_1_py = 365.25 * floods_1 / days,
            floods_2_py = 365.25 * floods_2 / days,
            floods_3_py = 365.25 * floods_3 / days,
            .groups = 'drop')
```

```{r predicted_floods}
summ <- slr_annual%>%
  summarize(across(contains('floods'), mean, na.rm = TRUE))
summ
```
That suggests a very large change in flooding frequency on the order of 
`r round(summ$floods_1/summ$floods_0,2)` with one foot of SLR.

# Simulation Models
We develop two simulation models that estimate future flooding by adding
simulated deviations to predicted tides. 

1.  Start with predicted tides from the official tidal epoch (in meters)  
2.  Add (or not) a sea level rise value to the predicted tides  
3.  Add a random deviation from predicted tidal elevation, where the random
    deviation is derived from either resampling from  hourly deviations or via
    simulating an ARIMA process.  
4.  Count up the number of days with simulated flood events over the tidal
    epoch, and divide by 365.25 to estimate floods per year.  
    
5.  In each Model, simulate 1000 mock tidal epochs, and look at properties of
    the resulting distribution of estimated rates of flooding per year. 

##  Resampling Model
The first model resamples deviations from the historically observed
deviations from predicted tides.

### Evaluating Resampled Deviations
We can simulate a "random"  time series by resampling from observed values of
the deviation between predicted and observed tidal levels. 
```{r test_resample}
tmp <-  epoch$deviation[! is.na(epoch$deviation)] # don't sample NAs
test <- sample(tmp, length(epoch$deviation), replace = TRUE)
```

```{r hist_test_resample}
ggplot() + 
  geom_histogram(aes(x = test, y = ..density..), bins = 100) +
  geom_density(aes(x = test), color = 'red') +
  stat_function(fun = dnorm, args = list(mean = mean(test), sd = sd(test)),
                color = 'blue') +
    geom_vline (xintercept = 0, color = 'red') 
```

Lets look at the moments of the simulated data. (Results will vary; this is a
simulation).
```{r moments_resample}
mean(test)
sd(test)
skewness(test)
kurtosis(test)
```
Moments are remarkably similar to source data, as would be expected.

```{r lines_test_resample, fig.width = 6}
ggplot() +

  geom_line(aes(x = (1:(24*50)), y = test[1:(24*50)]), alpha = 0.75) +
  geom_line(aes(x = (1:(24*50)), y = epoch$deviation[1:(24*50)]),
            color = 'orange', alpha = 0.75) +
   
  scale_x_continuous(breaks = seq(0, 24*100, 24*20)) +
  ylab('Deviances (m)') +
  xlab('Hours') +
  labs(subtitle = 'Black is Simulated, Orange is Observed')
```
The is a **lot** more temporal structure in the original data than in the
simulated data. 

### Resampling Function
This is the workhorse function that carries out the steps just described
(except the repeat sampling).
```{r resample_once_fxn}
hold_tmp <- NA
hold_df <- NA
resample_once <- function(dat, pr_sl, dev, dts, slr, flood = 1.987, 
                          testing = FALSE) {
  # dat is a dataframe
  # pr_sl is the data column containing the (astronomical) sea level predictions
  # dev is the data column containing observed deviations from predictions
  # dts is a data column of identifiers by date / day
  # slr is the value for SLR for estimating future tidal elevations 
  # flood is the selected flood level, here HAT = 1.987 meters
  # testing is a boolean that determines whether to save intermediates
  # Returns the mean number of floods per year over the simulated tidal epoch.
  
  # We quote data variables, and look them up by name
  # Caution:  there is no error checking.
  pr_sl <- as.character(ensym(pr_sl))
  dev <- as.character(ensym(dev))
  dts <- as.character(ensym(dts))
  

  tmp <-  dat[[dev]]
  tmp <-  tmp[! is.na(tmp)]
  
  #Simulate one tidal epoch of hourly tidal elevations
  val <- dat[[pr_sl]] + slr + sample(tmp,
                                     length(dat[[pr_sl]]),
                                     replace = TRUE)
  
  # create a dataframe, for convenient calculations
  df <- tibble(theDate = dat[[dts]], sim = val)
  if (testing) hold_df <<- df
  # Calculate results (DAYS with tidal elevations above flood, here HAT == 1.987 m)
  res <- df %>%
    group_by(theDate)  %>%
    summarize(exceeded = any(sim > flood),
              .groups = 'drop') %>%
    summarize(days = sum(! is.na(exceeded)),
              floods = sum(exceeded, na.rm = TRUE),
              floods_p_yr = 365.25 * floods/days) %>%
    pull(floods_p_yr)
  
  if (testing) {
    return( list(pred_floods = res, predicts = df))
  }
  else {
  return(res)
  }
}
```

### Test Our Function
```{r test_resample_once}
the_test <- resample_once(epoch, Prediction, deviation, theDate, 
              slr = 0, flood = 1.987,
              testing = TRUE)
the_test$pred_floods
```
Interestingly, the simulation consistently predicts about two and a half to
three times more flood days than were actually observed during the tidal epoch.
It is likely this discrepancy reflects the lack of tracking autocorrelation in
the deviations.  Effectively, with randomly sampled deviations, the probability 
on any given day of drawing a "large" deviation at close to high tide is
relatively high.  

Another cause, in contrast to the Portland, Maine simulations, is that the 
deviations represent a relatively higher proportion of the normal daily tidal elevations, thus increasing probability that a "random" deviation with tip
a prediction over the flood elevation.

### Run Full Simulation
The following takes about thirty seconds to run.
```{r demo_resample}
set.seed(12345)
samp = 1000
res <- numeric(samp)
for (iter in seq(1, samp)) {
  res[[iter]] <- resample_once(epoch, Prediction, deviation, theDate, 
                               slr = 0, flood = 1.987)
}
```

#### Evaluate Results
```{r moments_resample_results}
mean(res)
sd(res)
skewness(res)
kurtosis(res)
```

```{r hist_resample_results,fig.width = 4}
ggplot() +
  geom_histogram(aes(x = res, y = ..density..)) +
  geom_density(aes(x = res), color = 'red') +
  stat_function(fun = dnorm, args = list(mean = mean(res), sd = sd(res)),
                color = 'blue')
```

So, a resampling model suggests under conditions similar to the official tidal
epoch, we should have seen about nine flood events a year, considerably higher 
than we actually observed.

It is likely this discrepancy reflects the lack of autocorrelation in
the deviations.  Effectively, with randomly sampled deviations, the probability 
on any given day of drawing a "large" deviation at close to high tide is
relatively high.

Another cause, in contrast to the Portland, Maine simulations, is that the 
deviations represent a relatively higher proportion of the normal daily tidal 
elevations, thus increasing probability that a "random" deviation with tip
a prediction over the flood elevation.

## ARIMA Models
The prior analysis did not take into account the temporal autocorrelation of 
deviations from predicted tidal heights.  In this section, we take a similar
simulation approach, but simulate a slightly more faithful representation
of the distribution of deviations from predicted tidal heights, by simulating an 
ARIMA process.

### Developing an ARIMA Simulator 
We can fit an ARIMA model to the tidal deviations data, and then simulate random
deviations based on the parameters we find.  We fit an ARIMA model using the
`auto.arima()` function from the `forcast` package.

By definition, the observed deviations are stationary, with mean zero, so we
need not do any preliminary differencing.  We signal that to `auto.arima()` with
d = 0. We signal a stationary, non-seasonal process, with zero mean as well, to
take advantage of properties of the deviations.

Although our time series has a moderate "seasonal" structure at roughly a 
12 or 13 hour period (half of 25 hour, which is close to the tidal cycle of 24
hours 50 minutes), seasonal ARIMAs are very slow to fit. Even first order 
seasonal terms take four or five minutes to fit.  Second order and higher terms 
take substantially longer. An exhaustive search bogged down immediately.

We first search for non-seasonal ARIMA models, and then refit similar
seasonal models. 

We speed things up by setting `stepwise` and `approximation` arguments to
`FALSE`. You can request a trace to show how the model search progressed, as we
have done. You can also decide whether to fit seasonal terms, but to do that,
you have to pass a frequency to the timeseries object.

This takes around 20 seconds to run.  
```{r calculate_arima}
my_arima <- auto.arima(epoch$deviation,
                       d = 0,                # No differencing.
                       stationary = TRUE,
                       seasonal = FALSE,
                       allowdrift = FALSE,   # not sure how this differs from "stationary"
                       allowmean = FALSE,    # Our time series has mean = 0
                       stepwise = TRUE,      # FALSE is faster, less accurate
                       approximation = TRUE, # FALSE is slower
                       trace = TRUE)
#my_arima
(my_coefs <- coef(my_arima))
(my_sigma2 <- my_arima$sigma2)
```

That fits a fairly complex ARIMA model.

```{r acf_arima_residuals}
acf(my_arima$residuals,  na.action = na.pass, lag.max = 24*5)
pacf(my_arima$residuals,  na.action = na.pass, lag.max = 24*5)
```
so we have gotten rid of most of the non-periodic structure, and the structure
under a period of a few hours, but residuals still show higher-order periodic
structure, especially around 12 to 13 and 24 to 25 hours.

#### Taking into Account the Tides
We also try to fit a "Seasonal" model to better account for the tides.  To 
simplify our search, we focus on autoregressive models, dropping the moving
average terms.  There is no rational basis for that, except trying to simplify 
the models.

This takes on the order of 20 minutes to run.  We did not search other suitable
seasonal model components, and it is quite possible a moving average process
would be more successful.

The following takes on the order of 20 minutes to run.
```{r tidal_arima, cache = TRUE}
seasonal_arima <- arima(ts(epoch$deviation, frequency = 25),
                        order = c(5,0,0),
                        seasonal = list(order = c(2L, 0L, 0L)),
                        include.mean = FALSE
                        )
(seasonal_coefs <- coef(seasonal_arima))
(seasonal_sigma2 <- seasonal_arima$sigma2)
```

So that is only a slight improvement over the ARIMA(5,0,5) model, but it's 
substantially simpler too.

```{r acf_tidal_arima_residuals}
acf(seasonal_arima$residuals,  na.action = na.pass, lag.max = 24*5)
pacf(seasonal_arima$residuals,  na.action = na.pass, lag.max = 24*5)
```

This substantially reduces the periodic components in the ACF and 
PACF.  Magnitudes are reduced by about two thirds.

Unfortunately, base R and `forcast` have no suitable functions for simulating
seasonal ARIMA models, although some other packages apparently do. We do not
continue with exploration of simulation from a seasonal ARIMA model.

### Evaluating ARIMA Simulations
We can simulate a "random"  time series that matches the estimated ARIMA
structure of the observed deviations using `arima.sim()`.
```{r test_arima_sim}
test <- arima.sim(n = 365*24,
                  model = list(ar = my_coefs[1:5], ma = my_coefs[6:10]),
                  sd = sqrt(my_sigma2))
```

```{r histogram_arima_sim}
ggplot() +
  geom_histogram(aes(x = test, y = ..density..), bins = 100) +
  geom_density(aes(x = test), color = 'red') +
  stat_function(fun = dnorm, args = list(mean = mean(test), sd = sd(test)),
                color = 'blue') +
    geom_vline (xintercept = 0, color = 'red') 
```

The range of simulations is very close to the observed data.

#### Moments
Lets look at the moments of the simulated data. (Results will vary; this is a
simulation).
```{r moments_arima_sim}
mean(test)
sd(test)
skewness(test)
kurtosis(test)
```
Mean and SD are OK fits. 
Both skewness and kurtosis are lower than the historical data.  Skewness is 
almost an order of magnitude lower, and kurtosis about half. 

#### Autocorrelatons
We also examine temporal patterns and autocorrelation.
```{r lines_test_arima_sim, fig.width = 6}
ggplot() +
  geom_line(aes(x = (1:(24*100)), y = epoch$deviation[1:(24*100)]),
            color = 'orange', alpha = 0.5) +
  geom_line(aes(x = (1:(24*100)), y = test[1:(24*100)])) +
   
  scale_x_continuous(breaks = seq(0, 24*100, 24*20)) +
  ylab('Deviances (m)') +
  xlab('Hours') +
  labs(subtitle = 'Black is Simulated, Orange is Observed')
```

```{r acf_arima_sim}
acf(test, na.action = na.pass, lag.max = 24*5)
```

The overall temporal pattern of the simulation looks roughly comparable. The
autocorrelation structure shows a similar  initial decrease in correlation, but 
it does not level out at ACF ~ 0.2, as did the original data. The modeled values lack the periodic components of the real data.  There is somewhat more
long-term structure in the observed data, but we're not far off.

### Simulation Function
We create a function that simulates a possible timeseries of tidal heights. This
works in direct analogy to the resampling function, `resample_once()`. We
replace the resampling mechanism for generating a future stream of deviations 
from tidal predictions used there with one based on simulating an ARIMA process.

Note that this function relies on existence of parameters and standard error 
from an existing ARIMA model.  We pass those as parameters..

**Note this function expects coefficients from an ARIMA model of a
specific degree. WE modified this one to work with the ARIM(5,0,5) model fit
previously.** It may be better to standardize the degree of the ARIMA model 
we fit to the deviations data, precisely so we can standardize this function.
```{r sim_once_fxn}
sim_once <- function(dat, pr_sl, dts, slr, flood = 1.987,
                     coefs = my_coefs, 
                     sigma2 = my_sigma2) {
  
  # data is a dataframe
  # pr_sl is the data column containing the (astronomical) sea level predictions
  # dts is a data column of identifiers by date / day
  # slr is the value for SLR for estimating future tidal elevations 
  # coefs is a list of coefficients as produced by arima() or auto.arima()
  # sigma2 is the sigma2 measure of variation from arima() or  auto.arima()
  
  # Returns the mean number of floods per year over ONE simulated tidal epoch.
  
  # We quote data variables, and look them up by name
  # Caution:  there is no error checking.
  
  pr_sl <- as.character(ensym(pr_sl))
  #dev <- as.character(ensym(dev))
  dts <- as.character(ensym(dts))
  
  #Simulate one tidal epoch of hourly tidal elevations
  val <- dat[[pr_sl]] + slr + arima.sim(n = length(dat[[pr_sl]]),
                                 model = list(ar = coefs[1:5], 
                                              ma = coefs[6:10]),
                                 sd = sqrt(sigma2))
  
  #create a dataframe, for convenient calculations
  df <- tibble(theDate = dat[[dts]], sim = val)
  
  #Calculate results
  res <- df %>%
    group_by(theDate)  %>%
    summarize(exceeded = any(sim > flood),
              .groups = 'drop') %>%
    summarize(days = sum(! is.na(exceeded)),
              floods = sum(exceeded, na.rm = TRUE),
              floods_p_yr = 365.25 * floods/days) %>%
    pull(floods_p_yr)
  
  return(res)
} 

```

### Test Our Function
```{r test_sim_once}
sim_once(epoch, Prediction, theDate, 0, flood = 1.987)
```

That is MUCH close to the observed frequency of flooding over the official tidal
epoch.  That gives somewhat higher confidence in this model compared to teh resampling model.

### Run Full Simulation
The following takes just several minutes to run.
```{r demo_sim_once}
set.seed(54321)
samp = 1000
res <- numeric(samp)
for (iter in seq(1, samp)) {
  res[[iter]] <- sim_once(epoch, Prediction, theDate, 0)
}
```

#### Evaluate Results
```{r}
mean(res)
sd(res)
skewness(res)
kurtosis(res)
```
Estimated number of flood days per year is  lower, and in good agreement with 
the observed number during the official tidal epoch.

Results are close to normally distributed, with a slight hint of bimodality.
```{r sim_once_demo_hist, fig.width = 4}
ggplot() +
  geom_histogram(aes(x = res, y = ..density..), bins = 50) +
  geom_density(aes(x = res), color = 'red') +
  stat_function(fun = dnorm, args = list(mean = mean(res), sd = sd(res)),
                color = 'blue')
```

# Wrapping it all up
How much will SLR increase the frequency of days with flooding?  We compare
three methods.  In all cases, we focus on the period of the tidal epoch, when
difference between observed and predicted tides were minimized. That makes these
results only qualitatively related to change in frequency of flooding compared
to recent times.

## Direct Computation
Following the MGS method, but focused on the same period as our simulations for
comparison purposes.  Note these results are VERY different from results for
the last ten years, and in fact make the increase in flooding frequency look
much more severe.

Note we are working in meters here.

```{r direct}
HAT <- 1.987
slr_annual <- combined %>%
  filter(Year> 1982 & Year < 2002) %>%
  mutate(slr_1 = MLLW + 1 * 0.3048,
         slr_2 = MLLW + 2 * 0.3048,
         slr_3 = MLLW + 3 * 0.3048,
         exceeds_0 =  MLLW   > HAT, 
         exceeds_1 = slr_1   > HAT,
         exceeds_2 = slr_2   > HAT,
         exceeds_3 = slr_3   > HAT) %>%
  group_by(theDate) %>%
  summarize(exceeded_0 = any(exceeds_0, na.rm = TRUE),
            exceeded_1 = any(exceeds_1, na.rm = TRUE),
            exceeded_2 = any(exceeds_2, na.rm = TRUE),
            exceeded_3 = any(exceeds_3, na.rm = TRUE),
            .groups = 'drop') %>%
  
  summarize(days = sum(! is.na(exceeded_0)),
            `No SLR` = 365.25 * sum(exceeded_0)/days,
            `One Foot SLR` = 365.25 * sum(exceeded_1)/days,
            `Two Foot SLR` = 365.25 * sum(exceeded_2)/days,
            `Three Foot SLR` = 365.25 * sum(exceeded_3)/days,
            `One Foot` = round(`One Foot SLR` / `No SLR`, 2),
            `Two Foot` = round(`Two Foot SLR` / `No SLR`, 2),
            `Three Foot` = round(`Three Foot SLR` / `No SLR`, 2),
            .groups = 'drop') %>%
  select(-days)
slr_annual
(v <- slr_annual$`One Foot`)
```

Under that simple model, one foot of SLR increases flooding by a whopping 
`r round(v, 1)`. Interestingly, that is mostly because the
denominator -- the number of floods predicted under no sea level rise -- is so
small.  We use HAT as our definition of present-day flooding, but exceedences
above HAT would have been few during the Tidal Epoch, because tidal predictions
(including the definition of HAT) were based on observed tides during that 
period.

## Resampling
Create a function to allow us to use `map()` or `lapply()` to assemble results.

The function `res_auto()` is not fully encapsulated, since it relies on the 
existence of a data frame called "epoch", with data columns with specific names.
```{r res_auto fxn}
res_auto <- function(slr, samp) {
  res <- numeric(samp)
  for (iter in seq(1, samp))
    res[[iter]] <- resample_once(epoch, Prediction, deviation, theDate, slr)
  return(res)
}
  
```

The following takes a couple of minutes to run.  It simulates 1000
sets of tide levels for each of four SLR scenarios.  It takes a couple of 
minutes to  run.
```{r resamples}
set.seed(12345)
samp = 1000
resamp <- lapply((0:3 * 0.3048), function(x) res_auto(x, samp))
```

```{r}
names(resamp) = c('No SLR', 'One Foot SLR', 'Two Foot SLR', 'Three Foot SLR')
resamp <- do.call(bind_cols, resamp)
```

```{r}
resamp <- resamp %>% 
  summarize(across(everything(), mean, na.rm = TRUE)) %>%
  rowwise() %>%
  mutate(`One Foot`   = round(`One Foot SLR`/`No SLR`,2),
         `Two Foot`   = round(`Two Foot SLR`/`No SLR`,2),
         `Three Foot` = round(`Three Foot SLR`/`No SLR`,2))
resamp
```

```{r}
(v <- resamp$`One Foot`)
```

We can conclude that under this model, one foot of SLR will increase the 
frequency of flooding by something on the order of a factor of `r round(v,1)`. 
The analysis is conservative, to the extent that it is based on sea level
observations during the tidal epoch, which ended twenty years ago. The ratio
of increased flooding, however, is likely to be similar.

## ARIMA
We use the same approach here. Again, `sim_auto()` is not fully encapsulated.
```{r sim_auto_fxn}
sim_auto <- function(slr, samp) {
  res <- numeric(samp)
  for (iter in seq(1, samp))
    res[[iter]] <- sim_once(epoch, Prediction, theDate, slr)
  return(res)
}
  
```

The following takes several minutes to run.
```{r simulations}
set.seed(12345)
samp = 1000
simulates <- lapply((0:3 * 0.3048), function(x) sim_auto(x, samp))
```

```{r}
names(simulates) = c('No SLR', 'One Foot SLR', 'Two Foot SLR', 'Three Foot SLR')
simulates <- do.call(bind_cols, simulates)

simulates <- simulates %>%
  summarize(across(everything(), mean, na.rm = TRUE)) %>%
  rowwise() %>%
  mutate(`One Foot`   = round(`One Foot SLR`/`No SLR`,2),
         `Two Foot`   = round(`Two Foot SLR`/`No SLR`,2),
         `Three Foot` = round(`Three Foot SLR`/`No SLR`,2))
simulates

(v <- simulates$`One Foot`)
```

This analysis suggests a one foot sea level rise would increase flooding by a
factor of about `r round(v,1)`.

##Make a nice Table 
```{r}
t <- rbind(slr_annual, resamp, simulates)
row.names(t) <- c('Add SLR to Tidal Epoch', 'Resampling Model', 'ARIMA Model')
knitr::kable(t)
```



# Updated Simulation
Those simulations reflect conditions that held during the Tidal Epoch, from 1983
through 2001.  Today's sea levels are slightly higher.  How would taking that
into account affect predictions of the frequency of flooding and the relative
increase in frequency of flooding we  expect with one foot of *additional* SLR?

Actual tidal levels today are several centimeters higher than during the tidal
epoch.  With average SLR on the order of 3 mm per year, observed elevations
today should be on the order of 
$3.0 \frac{\text{mm}}{\text{yr}} \times 19\text{ yrs} = 57 \text{ mm} \approx 2.25 \text{ in}$.

How much will our results differ if we start from a base elevation a few inches
higher? We can compare frequency of flooding with an extra 6 cm of base
elevation versus flooding an additional 6cm plus one foot of SLR.

Again, this takes a few minutes to run.
```{r}
simulates <- lapply((.040 + (0:1 * 0.3048)), function(x) sim_auto(x, samp))
```

```{r}
names(simulates) = c('no_SLR', 'w_SLR')
simulates <- do.call(bind_cols, simulates)

simulates <- simulates %>%
  summarize(across(everything(), mean, na.rm = TRUE)) %>%
  rowwise() %>%
  mutate(`ratio`   = round(w_SLR/no_SLR,3))
simulates
```
So, just the SLR we have experienced over the past two decades has increased the
number of expected flooding events by about 75%, making the relative impact of
additional SLR look slightly smaller.
